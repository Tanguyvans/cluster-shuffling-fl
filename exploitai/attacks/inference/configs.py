from typing import Dict, Any, Optional
import torch


# ============================================================================
# DLG (Deep Leakage from Gradients) Configurations
# ============================================================================

def get_dlg_config(
    device: str = 'cpu',
    learning_rate: float = 0.1,
    max_iterations: int = 300,
    tv_reg: float = 0.001,
    l2_reg: float = 0.0,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Get configuration for DLG (Deep Leakage from Gradients) attack.

    DLG recovers training data by optimizing dummy inputs to match observed gradients.
    Best for: Basic gradient inversion, small batches, known architectures.

    Args:
        device: Device to run attack on ('cpu' or 'cuda')
        learning_rate: Learning rate for LBFGS optimizer
        max_iterations: Maximum optimization iterations
        tv_reg: Total variation regularization strength (for images)
        l2_reg: L2 regularization strength
        verbose: Whether to print progress

    Returns:
        Configuration dictionary for DLG attack
    """
    return {
        'attack_type': 'dlg',
        'device': device,
        'learning_rate': learning_rate,
        'max_iterations': max_iterations,
        'tv_reg': tv_reg,
        'l2_reg': l2_reg,
        'verbose': verbose,
        # DLG-specific parameters
        'optimizer': 'LBFGS',
        'batch_size_limit': 1,  # DLG works best with batch_size=1
    }


# ============================================================================
# GIAS (Gradient Inversion Attack on Speech) Configurations
# ============================================================================

def get_gias_config(
    device: str = 'cpu',
    learning_rate: float = 0.01,
    max_iterations: int = 1000,
    gias_iterations: int = 0,  # 0 means use max_iterations
    gias_lr: float = 0.01,
    cost_fn: str = 'sim_cmpr0',
    generator: Optional[str] = 'stylegan2',
    dataset: str = 'FFHQ64',
    num_restarts: int = 1,
    total_variation: float = 1e-1,
    image_norm: float = 1e-1,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Get configuration for GIAS (Gradient Inversion Attack on Speech) attack.

    GIAS uses generative models to reconstruct data from gradients.
    Best for: High-quality reconstruction, complex data distributions.

    Args:
        device: Device to run attack on
        learning_rate: Learning rate for latent space optimization
        max_iterations: Maximum iterations for latent space search
        gias_iterations: Iterations for parameter space search (0 = use max_iterations)
        gias_lr: Learning rate for parameter space optimization
        cost_fn: Cost function ('sim_cmpr0', 'l2', etc.)
        generator: Generative model ('stylegan2', 'biggan', etc.)
        dataset: Target dataset for generator
        num_restarts: Number of random restarts
        total_variation: TV regularization strength
        image_norm: Image normalization regularization
        verbose: Whether to print progress

    Returns:
        Configuration dictionary for GIAS attack
    """
    return {
        'attack_type': 'gias',
        'device': device,
        'learning_rate': learning_rate,
        'max_iterations': max_iterations,
        'gias_iterations': gias_iterations,
        'gias_lr': gias_lr,
        'cost_fn': cost_fn,
        'generator': generator,
        'dataset': dataset,
        'num_restarts': num_restarts,
        'total_variation': total_variation,
        'image_norm': image_norm,
        'verbose': verbose,
        # GIAS-specific parameters
        'gias': True,
        'optim': 'adam',
        'KLD': -1,  # Disable KL divergence
        'group_lazy': -1,  # Disable group lazy regularization
    }


# ============================================================================
# GIFD (Gradient Inversion from Federated Learning) Configurations
# ============================================================================

def get_gifd_config(
    device: str = 'cpu',
    learning_rate: float = 0.1,
    max_iterations: int = 1000,
    cost_fn: str = 'sim_cmpr0',
    generator: Optional[str] = 'stylegan2',
    dataset: str = 'FFHQ64',
    num_restarts: int = 1,
    total_variation: float = 1e-1,
    image_norm: float = 1e-1,
    bn_stat: float = 1e-1,
    group_lazy: float = 1e-1,
    z_norm: float = 0.0,
    inter_optimization: bool = True,
    verbose: bool = True
) -> Dict[str, Any]:
    """
    Get configuration for GIFD (Gradient Inversion from Federated Learning) attack.

    GIFD is the most advanced gradient inversion method with feature domain optimization.
    Best for: State-of-the-art reconstruction quality, federated learning scenarios.

    Args:
        device: Device to run attack on
        learning_rate: Learning rate for optimization
        max_iterations: Maximum optimization iterations
        cost_fn: Cost function ('sim_cmpr0', 'l2', 'gifd_loss', etc.)
        generator: Generative model ('stylegan2', 'biggan', etc.)
        dataset: Target dataset for generator
        num_restarts: Number of random restarts
        total_variation: TV regularization strength
        image_norm: Image normalization regularization
        bn_stat: Batch normalization statistics regularization
        group_lazy: Group lazy regularization strength
        z_norm: Latent code normalization
        inter_optimization: Whether to use inter-optimization
        verbose: Whether to print progress

    Returns:
        Configuration dictionary for GIFD attack
    """
    return {
        'attack_type': 'gifd',
        'device': device,
        'learning_rate': learning_rate,
        'max_iterations': max_iterations,
        'cost_fn': cost_fn,
        'generator': generator,
        'dataset': dataset,
        'num_restarts': num_restarts,
        'total_variation': total_variation,
        'image_norm': image_norm,
        'bn_stat': bn_stat,
        'group_lazy': group_lazy,
        'z_norm': z_norm,
        'inter_optimization': inter_optimization,
        'verbose': verbose,
        # GIFD-specific parameters
        'gifd': True,
        'optim': 'adam',
        'KLD': -1,  # Disable KL divergence for GIFD
        'restarts': num_restarts,
        'lr_decay': True,
        'init': 'randn',
        'signed': False,
        'indices': 'def',
        'weights': 'equal',
    }


# ============================================================================
# Preset Configurations for Common Scenarios
# ============================================================================

def get_cifar10_dlg_config(device: str = 'cpu') -> Dict[str, Any]:
    """Optimized DLG config for CIFAR-10 images."""
    return get_dlg_config(
        device=device,
        learning_rate=0.1,
        max_iterations=300,
        tv_reg=0.001,
        l2_reg=0.0
    )


def get_ffhq_gias_config(device: str = 'cuda') -> Dict[str, Any]:
    """Optimized GIAS config for FFHQ face images."""
    return get_gias_config(
        device=device,
        learning_rate=0.01,
        max_iterations=2000,
        gias_iterations=1000,
        gias_lr=0.01,
        cost_fn='sim_cmpr0',
        generator='stylegan2',
        dataset='FFHQ64',
        num_restarts=1,  # Reduced to 1 restart for faster execution
        total_variation=1e-2,
        image_norm=1e-2
    )


def get_inverting_gradients_config(
    device: str = 'cuda',
    learning_rate: float = 0.1,
    max_iterations: int = 10000,
    cost_fn: str = 'sim',  # Cosine similarity (Jonas Geiping method)
    optim_type: str = 'adam',
    restarts: int = 1,
    total_variation: float = 1e-3,
    image_norm: float = 1e-3,
    bn_stat: float = 1e-1,
    group_lazy: float = 1e-1
) -> Dict[str, Any]:
    """
    Get configuration for Inverting Gradients attack (Jonas Geiping et al.).

    Uses GIFD with cosine similarity cost function to replicate the original
    inverting gradients method from https://arxiv.org/abs/2003.14053

    This is essentially GIFD configured to use the same approach as the
    original inverting gradients paper.
    """
    return {
        'attack_type': 'gifd',  # Use GIFD with inverting gradients config
        'device': device,
        'learning_rate': learning_rate,
        'max_iterations': max_iterations,
        'cost_fn': cost_fn,  # 'sim' = cosine similarity (key to Jonas Geiping method)
        'generator': None,   # Pixel space optimization (no GAN)
        'dataset': None,
        'num_restarts': restarts,
        'total_variation': total_variation,
        'image_norm': image_norm,
        'bn_stat': bn_stat,
        'group_lazy': group_lazy,
        'z_norm': 0.0,
        'optim': optim_type,
        'indices': 'def',    # Use all parameters
        'weights': 'equal',  # Equal weighting
        'geiping': True,     # Enable Geiping method (GAN-free gradient inversion)
        'restarts': restarts
    }


def get_federated_gifd_config(device: str = 'cuda') -> Dict[str, Any]:
    """Optimized GIFD config for federated learning scenarios."""
    return get_gifd_config(
        device=device,
        learning_rate=0.1,
        max_iterations=2400,
        cost_fn='sim_cmpr0',
        generator='stylegan2',
        dataset='FFHQ64',
        num_restarts=3,
        total_variation=1e-1,
        image_norm=1e-1,
        bn_stat=1e-1,
        group_lazy=1e-1
    )


# ============================================================================
# Utility Functions
# ============================================================================

def validate_config(config: Dict[str, Any]) -> bool:
    """
    Validate a gradient inversion configuration.

    Args:
        config: Configuration dictionary to validate

    Returns:
        True if configuration is valid
    """
    required_keys = ['attack_type', 'device', 'learning_rate', 'max_iterations']

    for key in required_keys:
        if key not in config:
            raise ValueError(f"Missing required configuration key: {key}")

    if config['attack_type'] not in ['dlg', 'gias', 'gifd']:
        raise ValueError(f"Unknown attack type: {config['attack_type']}")

    if config['device'] not in ['cpu', 'cuda'] and not config['device'].startswith('cuda:'):
        # Allow cuda:X format
        if not (config['device'].startswith('cuda:') and config['device'][5:].isdigit()):
            raise ValueError(f"Invalid device: {config['device']}")

    return True


def print_config_summary(config: Dict[str, Any]) -> None:
    """
    Print a human-readable summary of the configuration.

    Args:
        config: Configuration dictionary to summarize
    """
    print(f"Attack Type: {config['attack_type'].upper()}")
    print(f"Device: {config['device']}")
    print(f"Learning Rate: {config['learning_rate']}")
    print(f"Max Iterations: {config['max_iterations']}")

    if config['attack_type'] == 'gias':
        print(f"GIAS Iterations: {config.get('gias_iterations', 'N/A')}")
        print(f"Generator: {config.get('generator', 'N/A')}")
        print(f"Dataset: {config.get('dataset', 'N/A')}")

    if 'cost_fn' in config:
        print(f"Cost Function: {config['cost_fn']}")

    if 'num_restarts' in config:
        print(f"Restarts: {config['num_restarts']}")


# ============================================================================
# Attack Factory Function
# ============================================================================

def create_attack_from_config(config: Dict[str, Any], model: torch.nn.Module):
    """
    Create an attack instance from a configuration dictionary.

    Args:
        config: Configuration dictionary
        model: Target model to attack

    Returns:
        Attack instance (DLGAttack, GIFDAttack, etc.)
    """
    validate_config(config)

    attack_type = config['attack_type']

    if attack_type == 'dlg':
        from .dlg import DLGAttack
        return DLGAttack(
            model=model,
            loss_fn=None,  # Uses default CrossEntropyLoss
            device=config['device'],
            learning_rate=config['learning_rate'],
            max_iterations=config['max_iterations'],
            tv_reg=config.get('tv_reg', 0.0),
            l2_reg=config.get('l2_reg', 0.0),
            verbose=config.get('verbose', True)
        )

    elif attack_type in ['gias', 'gifd']:
        try:
            from .gifd import GIFDAttack
        except ImportError:
            raise ImportError("GIFDAttack not available. Make sure gifd_core is properly installed.")
        return GIFDAttack(
            model=model,
            device=config['device'],
            generator=config.get('generator'),
            dataset=config.get('dataset', 'FFHQ64'),
            cost_fn=config.get('cost_fn', 'sim_cmpr0'),
            max_iterations=config['max_iterations'],
            learning_rate=config['learning_rate'],
            num_restarts=config.get('num_restarts', 1),
            total_variation=config.get('total_variation', 1e-1),
            image_norm=config.get('image_norm', 1e-1),
            bn_stat=config.get('bn_stat', 1e-1),
            group_lazy=config.get('group_lazy', 1e-1),
            z_norm=config.get('z_norm', 0.0),
            # Enable specific attack type
            gias=(attack_type == 'gias'),
            gifd=(attack_type == 'gifd')
        )

    else:
        raise ValueError(f"Unknown attack type: {attack_type}")
